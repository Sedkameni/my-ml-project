"""Sedrick_Project2-DA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SPaOz2pgSf_ebn7N8yfpVurU1GgW5ypJ

# 1 - Advanced Data Cleaning
*  Handle Missing Values: Employ advanced imputation techniques where appropriate, such as interpolation, using backward or forward filling, or model-based imputation.

 * Detect and Resolve Outliers: Apply robust methods to detect outliers in critical variables such as close prices and volume. Decide on a strategy to manage these outliers, whether it be removal, capping, or correction.

* Error Identification and Correction: Scan the dataset for errors introduced either through data entry or during initial data collection. Correct these errors to ensure data integrity.
"""

# Load the dataset and handle Missing values
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ------------------------------------------------
# 1. LOAD DATA FROM PROJECT 1
# ------------------------------------------------

# Define data types for memory optimization
stocks_dtypes = {
    'ticker': 'category',
    'exchange': 'category',
    'name': 'category',
    'sector': 'category',
    'industry': 'category'
}

prices_dtypes = {
    'ticker': 'category',
    'open': 'float32',
    'close': 'float32',
    'adj_close': 'float32',
    'low': 'float32',
    'high': 'float32',
    'volume': 'float32' # Using float32 for volume as it might contain NaNs or be interpolated
}

stocks_df = pd.read_csv("historical_stocks.csv", dtype=stocks_dtypes)
prices_df = pd.read_csv("historical_stock_prices.csv", dtype=prices_dtypes)

# Clean whitespace, convert to datetime safely, and drop bad rows
prices_df['date'] = (prices_df['date']
                      .astype(str)
                      .str.strip())
prices_df['date'] = pd.to_datetime(prices_df['date'], errors='coerce')
prices_df = prices_df.dropna(subset=['date'])

# Merge datasets
merged = prices_df.merge(stocks_df, on="ticker", how="left")

print("Merged dataset preview:")
print(merged.head())

"""# 2 - Data Transformation
*   Feature Engineering: Create new features that could enhance the model’s predictive power. This includes rolling averages, volatility measures, and technical indicators.
*   Data Normalization/Standardization: Standardize or normalize numerical fields to ensure that model inputs have similar scales.
* Encoding Categorical Variables: Apply appropriate encoding techniques such as one-hot encoding for categorical data.
"""

# =================================================================
# 2. ADVANCED DATA CLEANING
# =================================================================

# ------------------------------------------------
# 2.1 ADVANCED MISSING VALUE HANDLING
# ------------------------------------------------

# Check missing values
print("\nMissing values before cleaning:")
print(merged.isnull().sum())

# Forward fill numeric columns
num_cols = merged.select_dtypes(include=[np.number]).columns
merged[num_cols] = merged[num_cols].interpolate(method='linear')
#Missing close = 100 + (110 – 100)/2 = 105 - for linear


# Fill remaining missing categorical values with "Unknown"
cat_cols = merged.select_dtypes(include=['object']).columns
merged[cat_cols] = merged[cat_cols].fillna("Unknown")

print("\nMissing values AFTER cleaning:")
print(merged.isnull().sum())

# ------------------------------------------------
# 2.2 OUTLIER DETECTION & HANDLING
# Using IQR for 'close' and 'volume'
# ------------------------------------------------
def remove_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return df[(df[column] >= lower) & (df[column] <= upper)]
    #keeps only rows within the acceptable range.

print("Before outlier removal:", merged.shape)
#First, removes extreme stock prices (close column)
#Then, removes extreme trading volumes (volume column)
#After this, your dataset only contains realistic and meaningful data.
merged = remove_outliers_iqr(merged, 'close')
merged = remove_outliers_iqr(merged, 'volume')

print("After outlier removal:", merged.shape)

# ------------------------------------------------
# 2.3 ERROR IDENTIFICATION
# (1) Negative prices or volume
# ------------------------------------------------

# Replace negative values with NaN then interpolate
for col in ['open','high','low','close','volume']:
    merged.loc[merged[col] < 0, col] = np.nan
    merged[col] = merged[col].interpolate()

print("Any negative values left?")
print((merged[['open','high','low','close','volume']] < 0).sum())

print("Memory usage BEFORE optimization:")
print(merged.info(memory_usage='deep'))

# Optimize float columns from float64 to float32
float_cols = merged.select_dtypes(include=['float64']).columns
for col in float_cols:
    merged[col] = merged[col].astype('float32')

# Optimize 'ticker' column to category type
if 'ticker' in merged.columns:
    merged['ticker'] = merged['ticker'].astype('category')

print("\nMemory usage AFTER optimization:")
print(merged.info(memory_usage='deep'))

"""# 3 - Integration and Formatting for Modeling

*   Consolidate Data: Ensure all data transformations and cleanings are integrated into a single, coherent dataset ready for analysis.
*   Data Splitting: Prepare the data for predictive modeling by splitting it into training, validation, and test sets.

*  Save clean data: Save cleaned data and the splits for future use. 
"""

# ------------------------------------------------
# 3.1 FEATURE ENGINEERING
# ------------------------------------------------

# Rolling averages (technical indicators)
merged['ma_7'] = merged['close'].rolling(7).mean()
merged['ma_30'] = merged['close'].rolling(30).mean()
merged['volatility_30'] = merged['close'].rolling(30).std()

# Daily returns
merged['daily_return'] = merged['close'].pct_change()

# Future close price (for ML prediction)
merged['future_close_7'] = merged['close'].shift(-7)

merged.head()

# ------------------------------------------------
# 3.2 DATA NORMALIZATION / STANDARDIZATION
# ------------------------------------------------

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaled_cols = ['open','high','low','close','volume',
               'ma_7','ma_30','volatility_30','daily_return']

merged[scaled_cols] = scaler.fit_transform(merged[scaled_cols])

merged.head()

import gc

# ------------------------------------------------
# 3.3 ENCODING CATEGORICAL VARIABLES
# ------------------------------------------------

# One-hot encoding for industry, sector, exchange
cat_features = ['sector','industry','exchange']

# Check if 'merged' DataFrame exists before proceeding
if 'merged' in locals() or 'merged' in globals():
    merged_encoded = pd.get_dummies(merged, columns=cat_features, drop_first=True)

    # Free up memory by deleting the original 'merged' DataFrame
    del merged
    gc.collect()

    merged_encoded.head()
else:
    print("Error: 'merged' DataFrame is not defined. Please ensure preceding cells that create and process 'merged' have been executed.")

"""# 4 - Documentation and Reporting

*   Documentation: Create comprehensive documentation that details each step of the data cleaning and transformation process.
*   Reporting: Prepare a technical report that discusses the methodologies employed, the decisions made, and the rationale behind each decision. Include visualizations and statistical analyses that support the findings and demonstrate the transformations made to the data.
"""
# ------------------------------------------------
# 4.1 CONSOLIDATE CLEAN DATASET
# ------------------------------------------------

# Assign directly to avoid a full copy
clean_df = merged_encoded

# Optimize float columns from float64 to float32 to reduce memory footprint
float_cols = clean_df.select_dtypes(include=['float64']).columns
for col in float_cols:
    clean_df[col] = clean_df[col].astype('float32')

# Drop NaNs in place
clean_df.dropna(inplace=True)

# Free up memory by deleting merged_encoded if it's no longer needed separately
del merged_encoded
gc.collect()

print("Final cleaned dataset shape:", clean_df.shape)
clean_df.head()

# ------------------------------------------------
# 4.2 TRAIN / VALIDATION / TEST SPLIT
# Predict future_close_7
# ------------------------------------------------

from sklearn.model_selection import train_test_split

X = clean_df.drop(['future_close_7'], axis=1)
y = clean_df['future_close_7']

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, shuffle=False
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, shuffle=False
)

print("Train shape:", X_train.shape)
print("Validation shape:", X_val.shape)
print("Test shape:", X_test.shape)

# ------------------------------------------------
# 4.3 SAVE CLEAN DATA
# ------------------------------------------------

clean_df.to_csv("clean_stock_data.csv", index=True)
X_train.to_csv("train_X.csv")
y_train.to_csv("train_y.csv")
X_val.to_csv("val_X.csv")
y_val.to_csv("val_y.csv")
X_test.to_csv("test_X.csv")
y_test.to_csv("test_y.csv")

print("Files saved successfully! ")